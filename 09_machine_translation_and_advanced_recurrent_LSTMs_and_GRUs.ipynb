{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional MT Model\n",
    "Translation methods are statistical. It is impossible to compile a state machine for translation because there are so many different gramatical rules in a given language.\n",
    "\n",
    "The common approach is to use a parallel corpora, i.e. mapping one sentence to another sentence and then performing the following complicated steps.\n",
    "\n",
    "For example, source language is French `f` and target language is English `e`. The probablistic formulation using Bayes rule is,\n",
    "\n",
    "$$\n",
    "\\hat{e} = \\text{argmax}_{e}\\;P(e \\mid f) = \\text{argmax}_{e}\\;P(f \\mid e)P(e)\n",
    "$$\n",
    "\n",
    "* Translation model `P(f|e)` is trained on parallel corpus\n",
    "* Language model `P(e)` is trained on English on English corpus\n",
    "\n",
    "And then finally we have a decoder that takes these two models and smash them together that is trained on proper translation data.\n",
    "\n",
    "### Translation Model: Alignment\n",
    "The first step is to figure out which word or phrases in source language would translate to what words or phrases in target language. This is already a very hard problem.\n",
    "\n",
    "```python\n",
    "source = 'Japan shaken by two new quakes'\n",
    "target = 'Le Japon secoue par deux nouveaux seismes'\n",
    "```\n",
    "\n",
    "Notice that `Le` is not mapping to any English word because we don't say `The Japan`. Each phrase in source langauge has many possible translations resulting in a large search space.\n",
    "\n",
    "### Summary\n",
    "It has a lot of human feature engineering. It is a very complex system. It requires many different, independently trained machine learning models to perform one translation. Not to mention that there are hundreds of Human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Approach\n",
    "### Simple RNN\n",
    "Here's a simple approach, train a RNN as an encoder and splits out a vector at the end. Train another RNN as a decoder, take the output from the encoder and spit out a result.\n",
    "\n",
    "![rnn_machine_translation](./assets/rnn_machine_translation.png)\n",
    "\n",
    "#### Encoder\n",
    "Compute a hidden vector on each timestep using hidden vector from previous timestep and a word from current timestep.\n",
    "\n",
    "$$\n",
    "h_{t} = f\\left(W^{hh}h_{t-1} + W^{hx}x_{t}\\right)\n",
    "$$\n",
    "\n",
    "#### Decoder\n",
    "Compute a hidden vector on each timestep using hidden vector from previous timestep, \n",
    "\n",
    "$$\n",
    "h_{t} = f\\left(W^{hh}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "and then feed each timestep into a softmax.\n",
    "\n",
    "$$\n",
    "y_{t} = \\text{softmax}\\left(W^{S}h_{t}\\right)\n",
    "$$\n",
    "\n",
    "#### Objective\n",
    "Minimize cross entropy loss for all target words conditioned on source words.\n",
    "\n",
    "$$\n",
    "\\text{max}_{\\theta} \\frac{1}{N}\\Sigma^{N}_{n=1} \\text{log}\\; P_{\\theta}\\left(y^{n} \\mid x^{n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Extensions\n",
    "First, each encoder/decoder should have different weights for different languages. English encoder can be swapped out for a Chinese encoder and still work with the French decoder.\n",
    "\n",
    "#### Hidden State\n",
    "Each input of phi has its own linear transformation matrix.\n",
    "\n",
    "$$\n",
    "h_{t} = \\phi(h_{t-1}) = f\\left(W^{hh}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "Compute every hidden state in decoder from\n",
    "* Previous hidden state\n",
    "* Last hidden vector of encoder denoted as `c`\n",
    "* Previous predicted output word `y[t-1]`\n",
    "\n",
    "$$\n",
    "h_{D, t} = \\phi_{D}\\left(h_{t-1}, c, y_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "#### Additionals\n",
    "Train stacked/deep RNNs with multiple layers. Also consider training bi-directional encoder to avoid vanishing gradient problem. Or even train the input sequence in reverse order.\n",
    "\n",
    "#### Better Recurrent Unit\n",
    "The real improvement lies in solving the vanishing gradient problem. Vanilla RNN restrict us to short sequence of translation. We need a way to keep track memories better and provide a gradient highway. This will lead us to a better recurrent unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
