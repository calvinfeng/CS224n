{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Vectors for Word Representations\n",
    "## Skip-Gram Recap\n",
    "Let's recap the summary of Skip-Gram model. \n",
    "\n",
    "1. Go through each word of the whole corpus, suppose that `W` is the total number of words in the corpus, aka the vocabulary size. \n",
    "\n",
    "2. Predict surrounding words of each (window's center) word. The bottom sum can be computationally expensive cause it must go through the whole entire vocabulary.\n",
    "\n",
    "$$\n",
    "P(o \\mid c) = \\frac{exp(u_{o}^{T} v_{c})}{\\sum^{W}_{w=1} exp(u_{w}^{T}v_{c})}\n",
    "$$\n",
    "\n",
    "3. Take gradients at each such window for SGD\n",
    "\n",
    "There are two matrices, `V` and `U`. We will use upper cased letters to represent matrices and lower cased letters to represent a vector. Use `D` as the word vector feature dimension.\n",
    "\n",
    "`V` is the center word matrix. Notice that each column represents the word vector for one single word in the corpus.\n",
    "\n",
    "$$\n",
    "V = \\begin{bmatrix} \n",
    "V[0]_{0} & V[0]_{1} & ... & V[0]_{W} \\\\\n",
    "V[1]_{0} & V[1]_{1} & ... & V[1]_{W} \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "V[D]_{0} & V[D]_{1} & ... & V[D]_{W}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "`U` is the context word or outside word matrix. Similarly, each column represents the word vector for a single word in the corpus.\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix} \n",
    "U[0]_{0} & U[0]_{1} & ... & U[0]_{W} \\\\\n",
    "U[1]_{0} & U[1]_{1} & ... & U[1]_{W} \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "U[D]_{0} & U[D]_{1} & ... & U[D]_{W}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Objective**: We want to maximize the probability for each outside word, given a center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations of Words and Phrases and Their Compositionality\n",
    "The sum under probabiltity expression is very expensive and inefficient because for a given center word, most context words in the corpus are completely irrelevant to it. The dot product of two irrelevant words leads to zero contribution to the sum. \n",
    "\n",
    "*The trick here is to train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word.)*\n",
    "\n",
    "### Objective Function\n",
    "`T` is the total number of windows that we can possibly fit in a corpus given a window size. \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} J_{t}(\\theta)\n",
    "$$\n",
    "\n",
    "And the simplified cost function (which is a bit different from previous lecture) for a time step or also known as a given window is\n",
    "\n",
    "$$\n",
    "J_{t}(\\theta) = log \\; \\sigma(u_{o}^{T}v_{c}) + \\sum_{j \\tilde{} P(w)} \\left [ log \\; \\sigma(-u_{j}^{T}v_{c})\\right ]\n",
    "$$\n",
    "\n",
    "1. First term is using a sigmoid function instead of a typical probability notation because it is computationally easier to compute a sigmoid. The result will come out to be the same after maximization. \n",
    "2. The second term represents sub-sampling. We take `k` negative examples, i.e. random words that do not appear with the center word. \n",
    "3. Maximize probability that real outside word appears and minimize probability that random words appear around center word.\n",
    "\n",
    "### Subsampling Rule\n",
    "#### Math Trick\n",
    "$$\n",
    "\\sigma(-x) = 1 - \\sigma(x)\n",
    "$$\n",
    "\n",
    "#### Unigram Distribution `U(w)`\n",
    "The way we sample the random words are using unigram distribution `U(w)` raised to the $\\frac{3}{4}$ power. The power term makes less frequent words to be sampled more often. The unigram distribution function will be discussed later.\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{U(w)^{3/4}}{Z}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Based Approach\n",
    "Essentially the Skip-Gram model is capturing coocurrence of words one at a time. It answers the question how often do words appear next to each other. It's natural to think that, why not just use coocurrence matrix on words? For example, I have the following corpus.\n",
    "```\n",
    "I like deep learning.\n",
    "I like NLP.\n",
    "I enjoy driving.\n",
    "```\n",
    "\n",
    "Let's assume a window size of 1, which means we only count the nearest word. For example, if I have **I**, then I look at its neighboring word and do the count. In this case, the count for **like** is 2 because it appears twice near **I**.\n",
    "\n",
    "| counts   | I | like | enjoy | deep | learning | NLP | driving |\n",
    "|----------|---|------|-------|------|----------|-----|---------|\n",
    "| I        | 0 | 2    | 1     | 0    | 0        | 0   | 0       |   \n",
    "| like     | 2 | 0    | 0     | 1    | 0        | 1   | 0       |\n",
    "| enjoy    | 1 | 0    | 0     | 0    | 0        | 0   | 1       |\n",
    "| deep     | 0 | 1    | 0     | 0    | 1        | 0   | 0       |\n",
    "| learning | 0 | 0    | 0     | 1    | 0        | 0   | 0       |\n",
    "| NLP      | 0 | 1    | 0     | 0    | 0        | 0   | 0       |\n",
    "| driving  | 0 | 0    | 1     | 0    | 0        | 0   | 0       |\n",
    "\n",
    "There are couple disadvantages to doing this. \n",
    "\n",
    "* Increase in size with vocabulary\n",
    "* Very high dimensional because it is the square of vocabulary size\n",
    "* Subsequent classification models have sparsity issues\n",
    "\n",
    "However we can address those disadvantages with low dimensional vectors.\n",
    "\n",
    "* Store most of the important information in a fixed, small number of dimensions: a dense vector\n",
    "* Ideal dimension is probably around 25 ~ 1000\n",
    "* Reduce dimensionality using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (7, 7), singular values shape: (7,), V shape: (7, 7)\n",
      "[2.64  2.64  1.193 1.193 0.778 0.778 0.   ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"I\", \"like\", \"enjoy\", \"deep\", \"learning\", \"NLP\", \"driving\"]\n",
    "X = np.array([\n",
    "    [0, 2, 1, 0 ,0 ,0 ,0],\n",
    "    [2, 0, 0, 1, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "U, s, V = np.linalg.svd(X)\n",
    "\n",
    "print \"U shape: %s, singular values shape: %s, V shape: %s\" % (U.shape, s.shape, V.shape)\n",
    "print np.round(s, decimals=3)\n",
    "\n",
    "for i in xrange(len(words)):\n",
    "    plt.text(U[i, 0], U[i, 1], words[i])\n",
    "\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two columns of U corresponding to the 2 biggest singular values. There are many optimizations we can do to the SVD model. Certain words are too frequently appeared in the model. We can clamp the occurence to a maximum of 100 count or we can simply ignore them all. We can also use Pearson correlations instead of direct counts.\n",
    "\n",
    "The problem with SVD:\n",
    "* Computational cost scales quadratically, which is very bad for millions of words or documents.\n",
    "* Hard to incorporate new words or documents.\n",
    "\n",
    "| Count-based Approach: LSA, HAL, COALS, Hellinger-PCA | Direct Prediction: Skip-gram, NNLM, HLBL, RNN |\n",
    "|------------------------------------------------------|-----------------------------------------------|\n",
    "| Fast training | Scale with corpus size |\n",
    "| Efficient usage of statistics | Inefficient usage of statistics |\n",
    "| Primarily used to capture word similarity | Generate improved performance on other tasks |\n",
    "| Disproportionate importance given to large counts | Can capture complex patterns beyond word similarity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best of Both Worlds: GloVe\n",
    "*Pennington, Socher, Manning (2014)*\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\Sigma_{i, j = 1}^{W} f(P_{ij})(u_{i}^{T}v_{j} - log\\; P_{ij})^{2}\n",
    "$$\n",
    "\n",
    "* Fast training\n",
    "* Scalable to huge corpora\n",
    "* Good performance even with small corpus, and small vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
