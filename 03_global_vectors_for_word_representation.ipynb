{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Vectors for Word Representations\n",
    "## Skip-Gram Recap\n",
    "Let's recap the summary of Skip-Gram model. \n",
    "\n",
    "1. Go through each word of the whole corpus, suppose that `W` is the total number of words in the corpus, aka the vocabulary size. \n",
    "\n",
    "2. Predict surrounding words of each (window's center) word. The bottom sum can be computationally expensive cause it must go through the whole entire vocabulary.\n",
    "\n",
    "$$\n",
    "P(o \\mid c) = \\frac{exp(u_{o}^{T} v_{c})}{\\sum^{W}_{w=1} exp(u_{w}^{T}v_{c})}\n",
    "$$\n",
    "\n",
    "3. Take gradients at each such window for SGD\n",
    "\n",
    "There are two matrices, `V` and `U`. We will use upper cased letters to represent matrices and lower cased letters to represent a vector. Use `D` as the word vector feature dimension.\n",
    "\n",
    "`V` is the center word matrix. Notice that each column represents the word vector for one single word in the corpus.\n",
    "\n",
    "$$\n",
    "V = \\begin{bmatrix} \n",
    "V[0]_{0} & V[0]_{1} & ... & V[0]_{W} \\\\\n",
    "V[1]_{0} & V[1]_{1} & ... & V[1]_{W} \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "V[D]_{0} & V[D]_{1} & ... & V[D]_{W}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "`U` is the context word or outside word matrix. Similarly, each column represents the word vector for a single word in the corpus.\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix} \n",
    "U[0]_{0} & U[0]_{1} & ... & U[0]_{W} \\\\\n",
    "U[1]_{0} & U[1]_{1} & ... & U[1]_{W} \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "U[D]_{0} & U[D]_{1} & ... & U[D]_{W}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Objective**: We want to maximize the probability for each outside word, given a center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations of Words and Phrases and their Compositionality\n",
    "The sum under probabiltity expression is very expensive and inefficient because for a given center word, most context words in the corpus are completely irrelevant to it. The dot product of two irrelevant words leads to zero contribution to the sum. \n",
    "\n",
    "*The trick here is to train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word.)*\n",
    "\n",
    "### Objective Function\n",
    "`T` is the total number of windows that we can possibly fit in a corpus given a window size. \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} J_{t}(\\theta)\n",
    "$$\n",
    "\n",
    "And the simplified cost function (which is a bit different from previous lecture) for a given window is\n",
    "\n",
    "$$\n",
    "J_{t}(\\theta) = log \\; \\sigma(u_{o}^{T}v_{c}) + \\sum_{j \\tilde{} P(w)} \\left [ log \\; \\sigma(-u_{j}^{T}v_{c})\\right ]\n",
    "$$\n",
    "\n",
    "1. First term is using a sigmoid function instead of a typical probability notation because it is computationally easier to compute a sigmoid. The result will come out to be the same after maximization. \n",
    "2. The second term represents sub-sampling. We take `k` negative examples, i.e. random words that do not appear with the center word. \n",
    "3. Maximize probability that real outside word appears and minimize probability that random words appear around center word.\n",
    "\n",
    "**Math Trick**\n",
    "$$\n",
    "\\sigma(-x) = 1 - \\sigma(x)\n",
    "$$\n",
    "\n",
    "The way we sample the random words are using unigram distribution `U(w)` raised to the $\\frac{3}{4}$ power. \n",
    "\n",
    "$$\n",
    "P(w) = U(w)^{3/4}\n",
    "$$\n",
    "\n",
    "The power makes less frequent words to be sampled more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
