{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Models for Speech Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Speech Recognition\n",
    "\n",
    "You build a statistical model of speech starting from text sequences to audio features.\n",
    "\n",
    "![12_classic_speech_recognition](./assets/12_classic_speech_recognition.png)\n",
    "\n",
    "Each of the stage of the pipeline above uses a different statistical model.\n",
    "\n",
    "* Speech preprocessing uses a classical signal processing \n",
    "* Language model uses a n-gram model\n",
    "* Pronounciation uses a pronounciation table\n",
    "* Acoustic model uses a Gaussian mixture\n",
    "\n",
    "You look at the waveform, compute some features for it and then you look at your model and perform some inference to figure out what does it mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Although people later discovered that each of the model above can be improved significantly by using deep neural networks and recurrent neural networks, the problem is that these models may not fit well together. This drives people to seek an end-to-end model that performs all the tasks above in one-go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionist Temporal Classification\n",
    "\n",
    "Given an audio signal,\n",
    "\n",
    "$$\n",
    "X = x_{1}, x_{2}, ..., x_{T}\n",
    "$$\n",
    "\n",
    "where `x` is a frame of signal and a corresponding output text,\n",
    "\n",
    "$$\n",
    "Y = y_{1}, y_{2}, ..., y_{L}\n",
    "$$\n",
    "\n",
    "where `y` coud be a list of words or characters.\n",
    "\n",
    "We want to learn the probablistic model where `T` > `L`\n",
    "\n",
    "$$\n",
    "P(Y|X)\n",
    "$$\n",
    "\n",
    "`Y` is just a text sequence or transcript and `X` is the audio/processed spectrogram.\n",
    "\n",
    "![ctc](./assets/12_ctc.png)\n",
    "\n",
    "Here's how the frame predictions map to a output sequence. \n",
    "\n",
    "Each timestep can produce a symbol or letter through the softmax output. Some tokens may be duplicated like\n",
    "\n",
    "```\n",
    "cc<b>aa<b>t<b>\n",
    "```\n",
    "\n",
    "The original transcript maps to all possible paths in the duplicated space.\n",
    "\n",
    "```\n",
    "cc<b>aa<b>t<b> => cat\n",
    "cc<b><b>a<b>t<b> => cat\n",
    "cccc<b>aaaa<b>tttt<b> => cat\n",
    "cccccc<b>aa<b>tt<b> => cat\n",
    "```\n",
    "\n",
    "The score of any path is the sum of the score of individual categories at different time steps. The probability of any transcript is the sum of probabilities of all paths that correspond to that transcript. The `<b>` is known as the blank symbol.\n",
    "\n",
    "![ctc_prediction](./assets/12_ctc_prediction.png)\n",
    "\n",
    "### Language Model\n",
    "\n",
    "The end result will pronounce transcripts that sound correct, but lack the correct spelling and grammar rules. Although more training data can help, eventually a language model is required to fix these problems. With a simple language model rescoring, the word error rate goes from 30.1% to 8.7%. Google's CTC implementation fixes these problems by integrating a language model into CTC during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence \n",
    "\n",
    "In the CTC model, the model makes prediction based on only input data of a given frame. Once the prediction is made, there is no room to make adjustment. The next improvement we can make is to use sequence to sequence, passing a hidden state forward. Prediction for each timestep will factor in the predictions from previous timesteps and current waveform frame input.\n",
    "\n",
    "$$\n",
    "P(y_{i} \\mid y_{0..i}, x)\n",
    "$$\n",
    "\n",
    "The challenge with S2S training is that the sequence can be very long for audio streams. Each second is made up of 100 frames and for a 10 seconds audio input, it will have thousand of inputs. Even with LSTM, this can be quite stretching its limit. Therefore, we must use **attention** to guide where to look for the relevant input.\n",
    "\n",
    "![attention](./assets/12_attention.gif)\n",
    "\n",
    "### Listen, Attend, and Spell\n",
    "\n",
    "*Neural Machine Translation by Jointly Learning to Align and Translate*, 2015. \n",
    "\n",
    "First we have a bi-directional RNN as an encoder that acts as the listener. For every time step of the input, it produces some vector representation which encodes the input as `h[t]`. And then you generate the next character at every timestep with a decoder. You take the state vector of the decoder and compare it with each of the hidden time step of the encoder. The state vector is known as a **query**. We want to compute the similarity score between decoder query and encoder hidden state. The score is to tell us where to find the data we are looking for. This is where the attentions at.\n",
    "\n",
    "$$\n",
    "e_{t} = f\\left([h_{t}, s]\\right)\n",
    "$$\n",
    "\n",
    "![las_acoustic_model](./assets/12_las_acoustic_model.png)\n",
    "\n",
    "The decoder is also another recurrent neural network which computes the actual next word of the sequence by understanding where to look for that word from the audio signal.\n",
    "\n",
    "![las_architecture](./assets/12_las_architecture.png)\n",
    "\n",
    "### Limitation\n",
    "\n",
    "This is not an online model. All inputs must be received before transcripts can be produced. Attention is a computational bottleneck since every output token pays attention to every input time step. Length of the input has a big impact on accuracy as well.\n",
    "\n",
    "## Neural Transducer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
