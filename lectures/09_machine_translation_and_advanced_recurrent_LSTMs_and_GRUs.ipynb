{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional MT Model\n",
    "Translation methods are statistical. It is impossible to compile a state machine for translation because there are so many different gramatical rules in a given language.\n",
    "\n",
    "The common approach is to use a parallel corpora, i.e. mapping one sentence to another sentence and then performing the following complicated steps.\n",
    "\n",
    "For example, source language is French `f` and target language is English `e`. The probablistic formulation using Bayes rule is,\n",
    "\n",
    "$$\n",
    "\\hat{e} = \\text{argmax}_{e}\\;P(e \\mid f) = \\text{argmax}_{e}\\;P(f \\mid e)P(e)\n",
    "$$\n",
    "\n",
    "* Translation model `P(f|e)` is trained on parallel corpus\n",
    "* Language model `P(e)` is trained on English on English corpus\n",
    "\n",
    "And then finally we have a decoder that takes these two models and smash them together that is trained on proper translation data.\n",
    "\n",
    "### Translation Model: Alignment\n",
    "The first step is to figure out which word or phrases in source language would translate to what words or phrases in target language. This is already a very hard problem.\n",
    "\n",
    "```python\n",
    "source = 'Japan shaken by two new quakes'\n",
    "target = 'Le Japon secoue par deux nouveaux seismes'\n",
    "```\n",
    "\n",
    "Notice that `Le` is not mapping to any English word because we don't say `The Japan`. Each phrase in source langauge has many possible translations resulting in a large search space.\n",
    "\n",
    "### Summary\n",
    "It has a lot of human feature engineering. It is a very complex system. It requires many different, independently trained machine learning models to perform one translation. Not to mention that there are hundreds of Human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Approach\n",
    "### Simple RNN\n",
    "Here's a simple approach, train a RNN as an encoder and splits out a vector at the end. Train another RNN as a decoder, take the output from the encoder and spit out a result.\n",
    "\n",
    "![rnn_machine_translation](./assets/rnn_machine_translation.png)\n",
    "\n",
    "#### Encoder\n",
    "Compute a hidden vector on each timestep using hidden vector from previous timestep and a word from current timestep.\n",
    "\n",
    "$$\n",
    "h_{t} = f\\left(W^{hh}h_{t-1} + W^{hx}x_{t}\\right)\n",
    "$$\n",
    "\n",
    "#### Decoder\n",
    "Compute a hidden vector on each timestep using hidden vector from previous timestep, \n",
    "\n",
    "$$\n",
    "h_{t} = f\\left(W^{hh}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "and then feed each timestep into a softmax.\n",
    "\n",
    "$$\n",
    "y_{t} = \\text{softmax}\\left(W^{S}h_{t}\\right)\n",
    "$$\n",
    "\n",
    "#### Objective\n",
    "Minimize cross entropy loss for all target words conditioned on source words.\n",
    "\n",
    "$$\n",
    "\\text{max}_{\\theta} \\frac{1}{N}\\Sigma^{N}_{n=1} \\text{log}\\; P_{\\theta}\\left(y^{n} \\mid x^{n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Extensions\n",
    "First, each encoder/decoder should have different weights for different languages. English encoder can be swapped out for a Chinese encoder and still work with the French decoder.\n",
    "\n",
    "#### Hidden State\n",
    "Each input of phi has its own linear transformation matrix.\n",
    "\n",
    "$$\n",
    "h_{t} = \\phi(h_{t-1}) = f\\left(W^{hh}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "Compute every hidden state in decoder from\n",
    "* Previous hidden state\n",
    "* Last hidden vector of encoder denoted as `c`\n",
    "* Previous predicted output word `y[t-1]`\n",
    "\n",
    "$$\n",
    "h_{D, t} = \\phi_{D}\\left(h_{t-1}, c, y_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "#### Additionals\n",
    "Train stacked/deep RNNs with multiple layers. Also consider training bi-directional encoder to avoid vanishing gradient problem. Or even train the input sequence in reverse order.\n",
    "\n",
    "#### Better Recurrent Unit\n",
    "The real improvement lies in solving the vanishing gradient problem. Vanilla RNN restrict us to short sequence of translation. We need a way to keep track memories better and provide a gradient highway. This will lead us to a better recurrent unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "Standard RNN computes hidden vector at current timestep directly. GRU computes an **update** gate and a **reset** gate first.\n",
    "\n",
    "### Update Gate\n",
    "$$\n",
    "z_{t} = \\sigma\\left( W^{z}x_{t} + U^{z}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "`W` and `Z` are different weights that map to the same output hidden dimension.\n",
    "\n",
    "### Reset Gate\n",
    "$$\n",
    "r_{t} = \\sigma\\left( W^{r}x_{t} + U^{r}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "Similarly, reset gate produces a vector that has the same dimension as the output of update gate. The gate vectors are ranged from 0 to 1 due to the sigmoid function being applied.\n",
    "\n",
    "### Memory Content\n",
    "$$\n",
    "\\tilde{h} = \\text{tanh}\\left(Wx_{t} + r_{t}\\circ Uh_{t-1} \\right)\n",
    "$$\n",
    "\n",
    "The new memory content is denoted as tilde `h`. If reset gate is all zero, then the expression ignores the previous memory and only stores the new input (e.g. word) information. The final memory at time step combines the current and previous timesteps.\n",
    "\n",
    "$$\n",
    "h_{t} = z_{t} \\circ h_{t-1} + (1 - z_{t})\\circ \\tilde{h_{t}}\n",
    "$$\n",
    "\n",
    "Which is equivalent to\n",
    "\n",
    "$$\n",
    "h_{t} = z_{t} \\circ h_{t-1} + (1 - z_{t})\\circ \\text{tanh}\\left(Wx_{t} + r_{t}\\circ Uh_{t-1} \\right)\n",
    "$$\n",
    "\n",
    "### Intuition\n",
    "If reset is close to zero, ignore previous hidden state and allow model to drop information that is irrelevant to the future.\n",
    "\n",
    "Update gate controls how much of past state should matter now. If update is close to one, then we can copy information in that unit through many time steps and that means less vanishing gradient.\n",
    "\n",
    "Units with short-term dependencies often have reset gates very active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "The next level of complexity for gated units is long shot-term memory cell, known as the LSTM cells. We will have three different types of gate.\n",
    "\n",
    "### Input\n",
    "The inpute gate is measuring how much should the current cell state matter.\n",
    "\n",
    "$$\n",
    "i_{t} = \\sigma\\left( W^{i}x_{t} + U^{i}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "### Forget\n",
    "The forget gate is measuring how much should the previous cell state matter.\n",
    "\n",
    "$$\n",
    "f_{t} = \\sigma\\left(W^{f}x_{t} + U^{f}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "### Output\n",
    "The output gate is measuring how much should the current cell state be exposed to compute hidden vector of the current time step.\n",
    "\n",
    "$$\n",
    "o_{t} = \\sigma\\left(W^{o}x_{t} + U^{o}h_{t-1}\\right)\n",
    "$$\n",
    "\n",
    "### Cell State\n",
    "Every new cell state is a function of the previous cell state and previous hidden vector.\n",
    "\n",
    "$$\n",
    "c_{t} = i_{t} \\circ \\text{tanh}\\left( W^{c} x_{t} + U^{c}h_{t-1}\\right) +\n",
    "f_{t} \\circ c_{t-1}\n",
    "$$\n",
    "\n",
    "And then we can compute the hidden vector from the cell state.\n",
    "\n",
    "$$\n",
    "h_{t} = o_{t} \\circ \\text{tanh}\\left(c_{t}\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
