{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "\n",
    "## From RNN to CNN\n",
    "\n",
    "Recurrent neural networks cannot capture phrases without prefix context and often capture too much of last words in final vector. What if I just want to classify a simple phrase? \n",
    "\n",
    "> Example: the country of my birth\n",
    "\n",
    "We can use a sliding window of n-grams and perform convolution on them. We choose window of size 2, and we get `[the, country], [country, of], [of my], [my birth]`. We can also increase the window size to `3`, `4`, `5`, `...`, `N`. We can describe 1D convolution with the following equation.\n",
    "\n",
    "$$\n",
    "(f * g)[n] = \\Sigma^{M}_{m = -M} f[n-m]g[m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer CNN\n",
    "\n",
    "First introduced by the paper *Convolutional Neural Networks for Sentence Classification* by Kim in 2014. It is a simple variant using one convolutional layer and pooling layer to perform sentence classification. \n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "The inputs are word vectors of a sentence.\n",
    "\n",
    "$$\n",
    "x_{i} \\in \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "The sentence will be a concatenation of all the word vectors.\n",
    "\n",
    "$$\n",
    "x_{1:n} = x_{1} \\bigoplus x_{2} \\bigoplus ... \\bigoplus x_{n}\n",
    "$$\n",
    "\n",
    "Convolution filter will be denoted as `w`.\n",
    "\n",
    "$$\n",
    "w \\in \\mathbb{R}^{hk}\n",
    "$$\n",
    "\n",
    "Each window has`h` words each has `k` features. To compute the convolution, it's simply loops and summations.\n",
    "\n",
    "$$\n",
    "c_{i} = f\\left(w^{T}x_{i:i+h-1} + b\\right)\n",
    "$$\n",
    "\n",
    "The result is a feature map\n",
    "\n",
    "$$\n",
    "\\vec{c} = [c_{1}, c_{2}, ..., c_{n-h+1}] \\in \\mathbb{R}^{n-h+1}\n",
    "$$\n",
    "\n",
    "The problem is that the feature map will vary in length depending on the input sentence length. This needs to be addressed for CNN architecture.\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "Instead of sending the whole feature map to the next layer, we can use a pooling layer to select only the most activated feature. In this case, we want to pool a single number from a feature vector. This is different from what we do in CS231n. \n",
    "\n",
    "$$\n",
    "\\hat{c} = max\\{c\\}\n",
    "$$\n",
    "\n",
    "However, this wil only give us one feature. The next step to do is to introduce more filters. Each filter has different dimension. We perform convolution multiple times and perform max pooling on the input sentence. We will eventually end up with multiple features.\n",
    "\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The final feature vector will be a concatenation of the results from all max pooling layers.\n",
    "\n",
    "$$\n",
    "z = [\\hat{c}_{1}, \\hat{c}_{2}, ... \\hat{c}_{m}]\n",
    "$$\n",
    "\n",
    "We then feed it to a softmax layer.\n",
    "\n",
    "$$\n",
    "y = \\text{softmax}\\left(W^{S}z + b\\right)\n",
    "$$\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Here's how it looks like\n",
    "\n",
    "![13_conv_net_nlp](./assets/13_conv_net_nlp.png)\n",
    "\n",
    "### Hyperparamters in Kim (2014)\n",
    "\n",
    "* Nonlinearity uses ReLU\n",
    "* Window filter sizes `h = 3, 4, 5`\n",
    "* Each filter size has 100 feature maps\n",
    "* Dropout is 0.5\n",
    "* L2 constraints `s` for rows of softmax, `s = 3`\n",
    "* Mini-batch size for SGD training is 50\n",
    "* Word vectors are pretrained with word2vec, k = 300\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
