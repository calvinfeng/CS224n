{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU and Further Topics in NMT\n",
    "\n",
    "## How GRU Fix Things\n",
    "\n",
    "### Backpropagation through Time\n",
    "\n",
    "Vanishing gradient is a serious problem for basic recurrent neural networks. When the gradient becomes zero, we cannot tell whether\n",
    "\n",
    "1. Any dependency between `t` and `t+n` in data\n",
    "2. Any bad configuration of parameters\n",
    "\n",
    "Recall that forward propagation has the following form.\n",
    "\n",
    "$$\n",
    "f(h_{t - 1}, x_{t}) = \\text{tanh}(W(x_{t}) + Uh_{t-1} + b)\n",
    "$$\n",
    "\n",
    "The temporal derivative, i.e. with respect to state in time. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} = U^{T}\\frac{\\partial\\,\\text{tanh}(a)}{\\partial a}\n",
    "$$\n",
    "\n",
    "The gradient is multiplied by the weight matrix `U` per time step differentiation. If we have a long time sequence, then it is multiplied by `U` to N power. If the eigenvalue of `U` is big, then it is exploding gradient. If the eigenvalue of `U` is small, then it is vanishing gradient.\n",
    "\n",
    "#### Shortcut Connections\n",
    "\n",
    "This implies that the error must background through all the intermediate nodes. Perhaps we can create shortcut connections!\n",
    "\n",
    "![backprop thru time](./assets/11_backprop_thru_time.png)\n",
    "\n",
    "We want the shortcut such that `h[t]` can affect `h[t+2]` or `h[t+3]`, then we can measure the effect of `h[t+2]` on `h[t]`. \n",
    "\n",
    "Essentially that is what we are doing with the gated unit. It gives us the ability to create shortcuts *adaptively*. We enable the network to learn the strength of these shortcut connections.\n",
    "\n",
    "$$\n",
    "f(h_{t-1}, x_{t}) = u_{t}\\odot\\tilde{h_{t}} + (1 - u_{t}) \\odot h_{t-1}\n",
    "$$\n",
    "\n",
    "The candidate update is defined by the tilde h. \n",
    "\n",
    "$$\n",
    "\\tilde{h_{t}} = \\text{tanh}\\left(W[x_{t}] + U(r_{t} \\odot h_{t-1}) + b\\right)\n",
    "$$\n",
    "\n",
    "#### Update Gate\n",
    "\n",
    "The $u_{t}$ is the update gate which controls the strength of how much previous timestep should affect the currect timestep.\n",
    "\n",
    "$$\n",
    "u_{t} = \\sigma\\left(W_{u}[x_{t}] + U_{u}h_{t-1} + b_{u}\\right)\n",
    "$$\n",
    "\n",
    "#### Reset Gate\n",
    "\n",
    "We also need to let the network to prune unnecessary connections adaptively. So we have a reset gate.\n",
    "\n",
    "$$\n",
    "r_{t} = \\sigma\\left(W_{r}[x_{t}] + U_{r}h_{t-1} + b_{r}\\right)\n",
    "$$\n",
    "\n",
    "### Gradient Highway\n",
    "\n",
    "If we look at the equation again, the beauty is in the $(1 - u_{t})\\cdot h_{t-1}$ part.\n",
    "\n",
    "$$\n",
    "f(h_{t-1}, x_{t}) = u_{t}\\odot\\tilde{h_{t}} + (1 - u_{t}) \\odot h_{t-1}\n",
    "$$\n",
    "\n",
    "If the update gate is close to a vector of zeros, the current `h[t]` is directly reflecting `h[t-1]` which has a slope of 1. The information will flow directly forward without any new transformation. That is the perfect case for gradients to flow beautifully. This enables the network to establish long term dependency.\n",
    "\n",
    "On the other hand, if update gate is *learned* to be close to a vector of one, it implies the hidden states are being updated aggressively. That also means there is no need for long term dependency, feel free to let the gradients vanish!\n",
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "**NOTE**: The GRU and LSTM does not remember forever, the longest steps it can remember is around 100. It is called Long *Short Term Memory* for a reason.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "Let's compare GRU and LSTM and see their difference. GRU computes its hidden state by the following equations.\n",
    "\n",
    "- u is update gate\n",
    "- r is reset gate\n",
    "\n",
    "$$\n",
    "h_{t} = u_{t} \\odot \\tilde{h}_{t} + (1 - u_{t})\\odot h_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_{t} = \\text{tanh}\\left(W[x_{t}] + U(r_{t}\\odot h_{t-1}) + b\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_{t} = \\sigma\\left(W_{u}[x_{t}] + U_{u}h_{t-1} + b_{u}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{t} = \\sigma\\left(W_{r}[x_{t}] + U_{r}h_{t-1} + b_{r}\\right)\n",
    "$$\n",
    "\n",
    "The LSTM computes its hidden state by the following equations. \n",
    "\n",
    "- c is cell state\n",
    "- o is output gate\n",
    "- i is input gate\n",
    "- f is forget gate\n",
    "\n",
    "$$\n",
    "h_{t} = o_{t}\\odot\\text{tanh}(c_{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_{t} = i_{t} \\odot \\tilde{c_{t}} + f_{t}\\odot c_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{c_{t}} = \\text{tanh}\\left(W_{c}[x_{t}] + U_{c}h_{t-1} + b_{c}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_{t} = \\sigma\\left(W_{o}[x_{t}] + U_{o}h_{t-1} + b_{o}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_{t} = \\sigma\\left(W_{i}[x_{t}] + U_{i}h_{t-1} + b_{i}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{t} = \\sigma\\left(W_{f}[x_{t}] + U_{f}h_{t-1} + b_{f}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The hidden state of a GRU is equivalent to the cell state of a LSTM with a small difference.\n",
    "\n",
    "$$\n",
    "\\text{GRU}\\; h_{t} = u_{t} \\odot \\tilde{h_{t}} + (1 - u_{t})\\odot h_{t-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{LSTM}\\; c_{t} = i_{t} \\odot \\tilde{c_{t}} + f_{t}\\odot c_{t-1}\n",
    "$$\n",
    "\n",
    "### Secret Juice\n",
    "\n",
    "Rather than multiplying, we get `c[t]` by adding the non-linear stuff and `c[t-1]`. There is a direct linear connection between `c[t]` and `c[t-1]`. The same technique is also used in residual networks in computer vision, e.g. ResNet-50. When the network gets too deep for convolution, it suffers from the same vanishing gradient problem. The addition solves the problem!\n",
    "\n",
    "### Practical Advice\n",
    "\n",
    "1. Use an LSTM or GRU\n",
    "2. Initialize recurrent matrices to be orthogonal\n",
    "3. Initialize other matrices with small scale\n",
    "4. Initialize forget gate bias to 1 *default to remembering*\n",
    "5. Use adaptive learning rate algorithms, *Adam*, *AdaDelta*, ...\n",
    "6. Clip the norm of the gradient to 1 ~ 5\n",
    "7. Either only dropout vertically or learn how to do it right\n",
    "8. Be patient\n",
    "9. Do ensembles\n",
    "    - Train 8 ~ 10 nets and average their predictions\n",
    "    \n",
    "## Machine Translation Evaluation\n",
    "\n",
    "### BLEU\n",
    "\n",
    "BLEU stands for bilingual evaluation understudy. The idea is that have a human to produce a reference translation. We assume that the machine translation is good to the extend that you can find word n-grams, like three words in a row, two words in a row, etc...  which also appear in the reference translation anywhere. \n",
    "\n",
    "#### N-gram Precision\n",
    "\n",
    "The score lies between 0 and 1. It asks what percent of machine n-grams can be found in the reference translation? For each n-gram size, not a llowed to match identical portion of reference translation more than once. Commonly the n-gram is chosen to be 3 or 4 words.\n",
    "\n",
    "#### Brevity Penalty\n",
    "\n",
    "Machine cannot just type out single word `the`. If the translation is shorter than the human translation, there will be a penalty on the final score.\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "BLEU is a weighted geometric mean of n-gram precision with a brevity  penalty factor added.\n",
    "\n",
    "$$\n",
    "p_{n} = \\frac{\\text{number of matched n-gram}}{\\text{number of machine translation n-gram}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{n} = \\text{weight for each n-gram}\n",
    "$$\n",
    "\n",
    "$$\n",
    "BP = exp\\left(\\text{min}\\left(0, 1 - \\frac{len_{ref}}{len_{MT}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Then the BLEU score is calculated as follows.\n",
    "\n",
    "$$\n",
    "BLEU = BP \\prod^{N}_{n=1} p_{n}^{w_{n}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
