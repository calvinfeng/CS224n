{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU and Further Topics in NMT\n",
    "\n",
    "## How GRU Fix Things\n",
    "\n",
    "### Backpropagation through Time\n",
    "\n",
    "Vanishing gradient is a serious problem for basic recurrent neural networks. When the gradient becomes zero, we cannot tell whether\n",
    "\n",
    "1. Any dependency between `t` and `t+n` in data\n",
    "2. Any bad configuration of parameters\n",
    "\n",
    "Recall that forward propagation has the following form.\n",
    "\n",
    "$$\n",
    "f(h_{t - 1}, x_{t}) = \\text{tanh}(W(x_{t}) + Uh_{t-1} + b)\n",
    "$$\n",
    "\n",
    "The temporal derivative, i.e. with respect to state in time. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} = U^{T}\\frac{\\partial\\,\\text{tanh}(a)}{\\partial a}\n",
    "$$\n",
    "\n",
    "The gradient is multiplied by the weight matrix `U` per time step differentiation. If we have a long time sequence, then it is multiplied by `U` to N power. If the eigenvalue of `U` is big, then it is exploding gradient. If the eigenvalue of `U` is small, then it is vanishing gradient.\n",
    "\n",
    "#### Shortcut Connections\n",
    "\n",
    "This implies that the error must background through all the intermediate nodes. Perhaps we can create shortcut connections!\n",
    "\n",
    "![backprop thru time](./assets/11_backprop_thru_time.png)\n",
    "\n",
    "We want the shortcut such that `h[t]` can affect `h[t+2]` or `h[t+3]`, then we can measure the effect of `h[t+2]` on `h[t]`. \n",
    "\n",
    "Essentially that is what we are doing with the gated unit. It gives us the ability to create shortcuts *adaptively*. We enable the network to learn the strength of these shortcut connections.\n",
    "\n",
    "$$\n",
    "f(h_{t-1}, x_{t}) = u_{t}\\cdot\\tilde{h_{t}} + (1 - u_{t}) \\cdot h_{t-1}\n",
    "$$\n",
    "\n",
    "The candidate update is defined by the tilde h. \n",
    "\n",
    "$$\n",
    "\\tilde{h_{t}} = \\text{tanh}\\left(W[x_{t}] + U(r_{t} \\cdot h_{t-1}) + b\\right)\n",
    "$$\n",
    "\n",
    "#### Update Gate\n",
    "\n",
    "The $u_{t}$ is the update gate which controls the strength of how much previous timestep should affect the currect timestep.\n",
    "\n",
    "$$\n",
    "u_{t} = \\sigma\\left(W_{u}[x_{t}] + U_{u}h_{t-1} + b_{u}\\right)\n",
    "$$\n",
    "\n",
    "#### Reset Gate\n",
    "\n",
    "We also need to let the network to prune unnecessary connections adaptively. So we have a reset gate.\n",
    "\n",
    "$$\n",
    "r_{t} = \\sigma\\left(W_{r}[x_{t}] + U_{r}h_{t-1} + b_{r}\\right)\n",
    "$$\n",
    "\n",
    "### Gradient Highway\n",
    "\n",
    "If we look at the equation again, the beauty is in the $(1 - u_{t})\\cdot h_{t-1}$ part.\n",
    "\n",
    "$$\n",
    "f(h_{t-1}, x_{t}) = u_{t}\\cdot\\tilde{h_{t}} + (1 - u_{t}) \\cdot h_{t-1}\n",
    "$$\n",
    "\n",
    "If the update gate is close to a vector of zeros, the current `h[t]` is directly reflecting `h[t-1]` which has a slope of 1. The information will flow directly forward without any new transformation. That is the perfect case for gradients to flow beautifully. This enables the network to establish long term dependency.\n",
    "\n",
    "On the other hand, if update gate is *learned* to be close to a vector of one, it implies the hidden states are being updated aggressively. That also means there is no need for long term dependency, feel free to let the gradients vanish!\n",
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "**NOTE**: The GRU and LSTM does not remember forever, the longest steps it can remember is around 100. It is called Long *Short Term Memory* for a reason.\n",
    "\n",
    "The hidden state of a GRU is equivalent to the cell state of a LSTM with a small difference.\n",
    "\n",
    "$$\n",
    "\\text{GRU}\\; h_{t} = u_{t} \\cdot \\tilde{h_{t}} + (1 - u_{t})\\cdot h_{t-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{LSTM}\\; c_{t} = i_{t} \\cdot \\tilde{c_{t}} + f_{t}\\cdot c_{t-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
