{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Models\n",
    "\n",
    "Language model typically computes probability for sequences of words. For example, given a sequence, predict the next word of the input sequence and continue until a desired length is reached.\n",
    "\n",
    "### Markov Assumption\n",
    "\n",
    "Probability is usually conditioned on a window of `n` previous words. People tend to use Markov assumption which states that\n",
    "> The future is independent of the past given the present\n",
    "\n",
    "$$\n",
    "P(w_{1}, ... w_{m}) = \\prod P(w_{i} \\mid w_{i + 1 - n}, ..., w_{i-1})\n",
    "$$\n",
    "\n",
    "And to compute probabilities, we compute unigrams and bigrams (conditioning on one or two previous word(s))\n",
    "\n",
    "$$\n",
    "P(w_{2} \\mid w_{1}) = \\frac{count(w_{1}, w_{2})}{count(w_{1})}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "P(w_{3} \\mid w_{1}, w_{2}) = \\frac{count(w_{1}, w_{2}, w_{3})}{count(w_{1}, w_{2})}\n",
    "$$\n",
    "\n",
    "*n-gram is a contiguous sequence of n items from a given sample of text or speech*\n",
    "\n",
    "### Counting\n",
    "\n",
    "As one can already imagine that performance will increase as the `n` in n-gram increases. However, it is not enough to keep just one n-gram because some words usually occur in the beginning of a sentence so a robust model needs to try 5-gram, 4-gram and etc... This is so-called **backoff**, but it also means that it will take up a lot of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "Recurrent neural network addresses the RAM problem from the traditional language model. Its RAM requirement only scales with number of words.\n",
    "\n",
    "![rnn](./assets/08_rnn.png)\n",
    "\n",
    "Given a list of word vectors:\n",
    "\n",
    "$$\n",
    "x_{1}, ..., x_{t-1}, x_{t}, x_{t+1}, ..., x_{T}\n",
    "$$\n",
    "\n",
    "At every single time step `t`, we compute the a hidden vector.\n",
    "\n",
    "$$\n",
    "h_{t} = \\sigma\\left(W^{hh}h_{t-1} + W^{hx}x_{t}\\right)\n",
    "$$\n",
    "\n",
    "And an output vector.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t} = \\text{softmax}\\left(W^{S}h_{t}\\right)\n",
    "$$\n",
    "\n",
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_vec_dim = 10\n",
    "hidden_dim = 10\n",
    "output_dim = 5\n",
    "\n",
    "def get_word_vecs(corpus):\n",
    "    word_vecs = dict()\n",
    "    for word in corpus.split():\n",
    "        if word_vecs.get(word) is None:\n",
    "            word_vecs[word] = np.random.rand(word_vec_dim)\n",
    "    \n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 - np.exp(x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    shifted_logits = x - np.max(x, axis=0, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=0, keepdims=True)\n",
    "    return np.exp(shifted_logits) / Z\n",
    "\n",
    "\n",
    "def rnn_timestep(Whh, prev_h, Whx, x, Ws):\n",
    "    h = sigmoid(Whh.dot(prev_h) + Whx.dot(x))\n",
    "    y = softmax(Ws.dot(h))\n",
    "    \n",
    "    return h, y\n",
    "\n",
    "\n",
    "corpus = \"hello world this is natural language processing\"\n",
    "T = len(corpus.split())\n",
    "word_vecs = get_word_vecs(corpus)\n",
    "h0 = np.zeros(hidden_dim) # Use zero vector for initial hidden\n",
    "Whh = np.random.rand(hidden_dim, hidden_dim)\n",
    "Whx = np.random.rand(hidden_dim, word_vec_dim)\n",
    "Ws = np.random.rand(output_dim, hidden_dim)\n",
    "\n",
    "hs = (T + 1) * [h0] # Hidden vectors for all timesteps\n",
    "ys = (T + 1) * [None] # Output vectors for all timesteps\n",
    "for i, word in enumerate(corpus.split()):\n",
    "    t = i + 1\n",
    "    x = word_vecs.get(word)\n",
    "    hs[t], ys[t] = rnn_timestep(Whh, hs[t-1], Whx, x, Ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "*I am not going to do the backprop here because I have a blog post about this on my machine learning notebook already.* We can use the same cross entropy loss as before to compute loss at each timestamp. Here I use `V` to represent the size of vocabulary.\n",
    "\n",
    "$$\n",
    "J_{t}(\\theta) = - \\Sigma_{j = 0}^{V} y_{t}\\text{[j]} \\;\\text{log}\\; \\hat{y}_{t}\\text{[j]}\n",
    "$$\n",
    "\n",
    "Thus, the total loss is just averaging all the timestep losses.\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{T} \\Sigma^{T}_{t=1} J_{t}\n",
    "$$\n",
    "\n",
    "More commonly use is the perplexity score which is\n",
    "\n",
    "$$\n",
    "\\text{Perplexity:} \\; 2^{J}\n",
    "$$\n",
    "\n",
    "Training RNN is actually hard because of vanishing gradient. This can be addressed using LSTM in next lecture.\n",
    "\n",
    "### Truncated Backpropagation\n",
    "\n",
    "Remember that we only have three weight matrices, `Whh`, `Whx`, and `Ws`. It is rather inefficient to perform an update on these matrices at every timestep. The better approach is that we parse the corpus into different sequences. At every sequence, we reset the hidden vector back to zero, we move along the timesteps and accumulate all the gradients at each timestep. For example,\n",
    "\n",
    "```python\n",
    "hs = (seq_len + 1) * [None]\n",
    "hs[0] = # set zero vector\n",
    "grad_Whh, grad_Whx, grad_Ws = # initialize as zero matrices\n",
    "for i in range(sequence_length):\n",
    "    t = i + 1\n",
    "    grads = model.loss(x[t], y[t], h[t-1])\n",
    "    grad_Whh += grads['Whh']\n",
    "    grad_Whx += grads['Whx']\n",
    "    grad_Ws += grads['Ws']\n",
    "```\n",
    "\n",
    "And then we perform an update on `Whh`, `Whx`, and `Ws` in one go and move onto the next sequence. This is known as **truncated** backpropagation. It is very much like stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Deep RNN\n",
    "For classification problem, sometimes you actually want to incorporate information from words both proceding and following. The vanilla RNN will only read words from left to right. The current prediction relies on the information from the past. What if the classification of a word relies on the words that come before and after it? We can use bidirectional RNN.\n",
    "\n",
    "![bidirectional_rnn](./assets/08_bidirectional_rnn.png)\n",
    "\n",
    "Instead of one hidden vector for prediction, we use two! The new hidden vector now summarizes the past and future around a single token. If that's not enough, try to go deep and make multiple layers with multiple hidden vectors at each timestep.\n",
    "\n",
    "![deep_bidirectional_rnn](./assets/08_deep_bidirectional_rnn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
