{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Window Classification and Neural Networks\n",
    "\n",
    "## Background - Cross Entropy\n",
    "It's good to explain what is cross entropy despite that we have seen it and used it many times. The entropy is measuring the disorder or non-uniformity of two probability distributions. Which two? We have one for probability distribution from the model we are training and one probability distribution from the training labels. \n",
    "\n",
    "We have a ground truth probability distribution from label data. \n",
    "```python\n",
    "p = [0, 0, 0, ..., 1, ..., 0]\n",
    "```\n",
    "\n",
    "Our computed probability is `q[c]` for a given class `c`. Then we can say that cross entropy is the following.\n",
    "$$\n",
    "H(p, q) = -\\Sigma_{c=1}^{C} p(c)\\;log\\;q(c)\n",
    "$$\n",
    "\n",
    "**Before p is an one-hot vector, the only term left is the negative probability of the true class.** The objective function is trying to minimize the Kullback-Leibler divergence between the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy: 0.327756\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_classes = 10\n",
    "y = 4\n",
    "\n",
    "true_dist = signal.unit_impulse(num_classes, y)\n",
    "rand_dist = np.random.rand(10,)\n",
    "plt.plot(np.arange(0, 10), true_dist, '-o', rand_dist, '-o')\n",
    "plt.show()\n",
    "\n",
    "print 'cross entropy: %f' % (-1 * np.log(rand_dist[y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Regularization\n",
    "Also don't forget the regularization term because it is essential to prevent overfitting, plus it is not a difficult thing to incorporate into a training objective.\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{N}\\Sigma^{N}_{i=1} -log\\left(P(f_{y[i]})\\right) + \\lambda \\Sigma_{k} \\theta^{2}_{k}\n",
    "$$\n",
    "\n",
    "##  Re-training Word Vectors\n",
    "* If you only have a small training data set, don't train the word vectors.\n",
    "* If you have a very large dataset, it may work better to train word vectors the task.\n",
    "\n",
    "## Window Classification\n",
    "\n",
    "### Ambuigity\n",
    "Classifying a single word is rarely done this is because the meaning of a word varies depending on context. There is a lot of ambiguity to a single word.\n",
    "\n",
    "**Example**: auto-antonyms\n",
    "* \"To sanction\" can mean \"to permit\" or \"to punish\"\n",
    "* \"To seed\" can mean \"to place seeds\" or \"to remove seeds\"\n",
    "\n",
    "**Example**: ambiguous named entities\n",
    "* Paris could mean Paris, France or Paris Hilton\n",
    "* Hathaway could mean Berkshire Hathaway or Anne Hathaway\n",
    "\n",
    "### Idea\n",
    "We want to classify a word in its context window of neighboring words. For example, named dentity recognition can be classified into four classes.\n",
    "* Person, location, organization and none\n",
    "\n",
    "There are many possibilities for classifying one word in context, e.g. averaging all the words in a window but that loses position information. We will train a softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it.\n",
    "\n",
    "**Example**: classify Paris in the context of this sentence with window length 2.\n",
    "$$\n",
    "X_{window} = \\left[ x_{museums}, x_{in}, x_{Paris}, x_{are}, x_{amazing} \\right]\n",
    "$$\n",
    "\n",
    "The resulting vector is of length `5*D` where `D` is the word vector dimension.\n",
    "\n",
    "### Implementation\n",
    "Before we implement anything, let's define the terminology concretely here.\n",
    "\n",
    "We use y hat to denote the prediction.\n",
    "$$\n",
    "\\hat{y}: \\text{softmax probability output vector}\n",
    "$$\n",
    "\n",
    "Target probability distribution is the ground truth vector. All 0's except ground truth index where it's 1.\n",
    "$$\n",
    "t: \\text{target probability distribution}\n",
    "$$\n",
    "\n",
    "We use simple affine transformation to feed forward. \n",
    "$$\n",
    "f = f(x) = xW\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = \"Calvin and Carmen are going to Korea soon\".split()\n",
    "# Define couple dimension variables\n",
    "# N for number of windows, C for number of classes, and D for vector dimension for each window.\n",
    "C = 4\n",
    "D = 300\n",
    "N = len(corpus)\n",
    "\n",
    "# I go through each word of the corpus and generate a random word vector for it.\n",
    "V = dict()\n",
    "for word in corpus:\n",
    "    V[word] = np.random.randn(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule of thumb is that, use word to random vec when the dataset is small, use Skip-gram model word to vec when dataset is huge. Now we are can slide across the whole corpus and create windows. I will use zero vectors to represent the words that do not exist or for the portion of the window that is out of bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(corpus)):\n",
    "        vecs = []\n",
    "        for j in range(i - 2, i + 3):\n",
    "            if j < 0 or len(corpus) <= j:\n",
    "                vecs.append(np.zeros((D)))\n",
    "            else:\n",
    "                vecs.append(V[corpus[j]])\n",
    "        \n",
    "        X.append(np.concatenate(tuple(vecs), 0))\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `(N, 5D)` tensor, where N is the number of windows and D is the word vector dimension for an individual word. We now just need to define a weight and perform feedforward operation on it. Let's first define the numerically stable softmax probability function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_probability(x):\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    return np.exp(shifted_logits) / Z\n",
    "\n",
    "W = np.random.rand(5*D, C)\n",
    "f = np.dot(X, W)\n",
    "y = stable_probability(f) # This is the final output of the model, a matrix of softmax probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagations\n",
    "We have done this many times already. I will just take the code from my other project and use it here. Let's assume that the ground truth distribution is the following. I will use the letter `t` to denote target distribution or target label etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss is 7.197643\n"
     ]
    }
   ],
   "source": [
    "# Four classes: person, location, organization, and none\n",
    "t = np.array([[1, 0, 0, 0],\n",
    "            [0, 0, 0, 1],\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 0, 0, 1],\n",
    "            [0, 0, 0, 1],\n",
    "            [0, 0, 0, 1],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 0, 1]])\n",
    "\n",
    "# Let's compute the loss first.\n",
    "def categorical_cross_entropy(y_pred, y):\n",
    "    \"\"\"Computes categorical cross entropy loss.\n",
    "    Args:\n",
    "        y_pred (numpy.ndarray): Output of the network, of shape (N, C) where x[i, j] is the softmax \n",
    "                                probability for for jth class for the ith input.\n",
    "        y (numpy.ndarray): Vector of labels in one-hot representation.\n",
    "    Returns:\n",
    "        loss (float): Scalar value of the cross entropy loss.\n",
    "    \"\"\"\n",
    "    N = len(y_pred)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    log_probs = np.log(y_pred)\n",
    "\n",
    "    return -1 * np.sum(log_probs[np.arange(N), y]) / N\n",
    "\n",
    "print 'Cross entropy loss is %f' % categorical_cross_entropy(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do backpropagation and reduce minimize this loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(1500, 4)\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of f\n",
    "grad_f = y.copy()\n",
    "grad_f[np.arange(N), np.argmax(t, axis=1)] -= 1\n",
    "grad_f /= N\n",
    "print grad_f.shape # (N, C)\n",
    "\n",
    "grad_W = np.dot(X.T, grad_f) # (D, N)(N, C) => (D, C)\n",
    "print grad_W.shape # (D, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put everything together and run 100 iterations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = []\n",
    "for i in range(100):\n",
    "    # Forward\n",
    "    f = np.dot(X, W)\n",
    "    y = stable_probability(f)\n",
    "    loss_history.append(categorical_cross_entropy(y, t))\n",
    "    \n",
    "    # Backprop\n",
    "    grad_f = y.copy()\n",
    "    grad_f[np.arange(N), np.argmax(t, axis=1)] -= 1\n",
    "    grad_f /= N\n",
    "    grad_W = np.dot(X.T, grad_f)\n",
    "    \n",
    "    # Weight update\n",
    "    W -= 5e-3 * grad_W\n",
    "\n",
    "plt.plot(np.arange(100), loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Margin Loss\n",
    "This loss is often more robust and more powerful than cross entropy loss for our task here. The training objective is to make score of true window larger and score of corrupt window lower. \n",
    "$$\n",
    "J = max(0, 1 - s + s_{c})\n",
    "$$\n",
    "\n",
    "For example,\n",
    "```python\n",
    "s = score(\"museums in Paris are amazing\") \n",
    "s_c = score(\"Not all museums in Paris\") # corrupted window\n",
    "```\n",
    "\n",
    "We want to give the window where the center word is a location a higher score than the window where the center word is NOT a location. This is essentially a **hinge loss**. We sample several corrupt windows per true one. Sum over all training windows. The corrupt window is just a negative class.\n",
    "\n",
    "### Gradient\n",
    "Actually the gradient of a hinge loss is very similar to that of **ReLU**. It's an on/off gate. Suppose that we group the positive and negative class score together and call it delta s.\n",
    "$$\n",
    "\\Delta s = 1 - s + s_{c}\n",
    "$$\n",
    "\n",
    "Then the loss function is simply.\n",
    "$$\n",
    "J = max(0, \\Delta s)\n",
    "$$\n",
    "\n",
    "If delta s is greater than zero, then the gradient is 1.\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\Delta s} = 1\n",
    "$$\n",
    "\n",
    "Else it is zero.\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\Delta s} = 0\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
