{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "Woo, it's tensorflow again. Let's build some graphs!\n",
    "\n",
    "## Programming Model\n",
    "Tensorflow expresses a numeric computation as a graph. Each node is an operation which has any number of inputs and outputs. Each edge is a tensor which flows between nodes. Suppose we have a simple network expressed by the following operation(s).\n",
    "\n",
    "$$\n",
    "h = \\text{ReLU}(Wx + b)\n",
    "$$\n",
    "\n",
    "We can interpret it as three sequential operations.\n",
    "![tf-graph](./assets/tf-graph.png)\n",
    "\n",
    "## Node Types\n",
    "New nodes are automatically built into the underlying graph. We can inspect what are the nodes inside our graph using `get_operations` on our default graph. As we can see there are multiple types of node.\n",
    "```\n",
    "tf.get_default_graph().get_operations()\n",
    "```\n",
    "\n",
    "### Variables\n",
    "Variables are stateful nodes which output their current value. State is retained across multiple executions of a graph. We can think of variables as the parameters we wish to tune. The training will be occurred on variables instead of placeholders.\n",
    "```python\n",
    "b = tf.Variable(tf.zeros((100,)))\n",
    "W = tf.Variable(tf.random_uniform((784, 100), -1, 1))\n",
    "```\n",
    "\n",
    "### Placeholder\n",
    "Placeholders are nodes whose value is fed in at execution time.\n",
    "```python\n",
    "x = tf.placeholder(tf.float32, (100, 784))\n",
    "```\n",
    "\n",
    "### Mathematical operation\n",
    "For example, we have `MatMul`, `Add`, `ReLU` and the list goes on.\n",
    "```python\n",
    "h = tf.nn.relu(tf.matmul(x, W), + b)\n",
    "```\n",
    "\n",
    "## Session\n",
    "Once a graph is defined, we can deploy it with a session, which is a binding to a particular execution context. The `run` method has the followng signature.\n",
    "```\n",
    "sess.run(fetches, feeds)\n",
    "```\n",
    "\n",
    "### Arguments\n",
    "There are two arguments we need to supply to a session.\n",
    "* Fetches - List of graph nodes which will return outputs of these nodes\n",
    "* Feeds - Dictionary mapping from graph nodes to concrete values which will specify the value of placeholder.\n",
    "\n",
    "```python\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(h, {x: np.random.random(100, 784)})\n",
    "```\n",
    "\n",
    "### Losses\n",
    "Since labels are not parameters we are going to tune, we will use placeholder for them. In fact, loss is just a mathematical operation node.\n",
    "```python\n",
    "prediction = tf.nn.softmax(...) # Output of a neural network\n",
    "label = tf.placeholder(tf.float32, [100, 10])\n",
    "cross_entropy = -tf.reduce_sum(label * tf.log(prediction), axis=1)\n",
    "```\n",
    "\n",
    "### Optimizer\n",
    "It's time to compute gradients! We need to first define an optimizer object. This should be a familiar concept to me, I have written many different types of optimizer in CS231n. \n",
    "```python\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "```\n",
    "\n",
    "Each graph node has attached gradient operation. The gradient operation computes local gradient and combines it with upstream gradient with respect to loss. In order to use the gradient to update our parameters, we simply run the training step in a session.\n",
    "```python\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    batch_x, batch_y = data.next_batch()\n",
    "    sess.run(train_step, feed_dict={x: batch_x, label: batch_y})\n",
    "```\n",
    "\n",
    "### Shared Variables\n",
    "What if you are running multiple sessions on a cluster of computers but you are still training one model, i.e. the same set of variables is being used across multiple machines/sessions? We can use `variable_scope`!\n",
    "```python\n",
    "with tf.variable_scope('foo'):\n",
    "    v = tf._get_variable('v', shape=[1]) # v.name == \"foo/v:0\"\n",
    "   \n",
    "with tf.variable_scope('foo', reuse=True):\n",
    "    v1 = tf.get_variable('v') # shared variable found!\n",
    "   \n",
    "with tf.variable_scope('foo', reuse=False):\n",
    "    v1 = tf.get_variable('v') # CRASH foo/v:0 already exists\n",
    "```\n",
    "\n",
    "`variable_scope()` provides simple name-spacingto avoid clashes. `get_variable()` creates/accesses variables from within a variable scope. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
